---
title: "EDA Crime Lab"
author: "Eduarda Espindola, Laura Pintos, Payman Roghani and Pri Nonis"
date: "20/11/2018"
output: pdf_document
---

# Crime Lab

_Eduarda, Laura, Payman, Pri_


## Introduction

A lot has been said about crime and its drivers, and the subject is always a concern for policy makers. We are proposing a more data driven approach to the subject, in order to assist the policy makers building a more assertive agenda towards reducing crime rates. While we believe there are many variables affecting crime rates, we choose to focus on those which are easier to change in a shorter period of time, and thus possibly reducing crime rate faster.

## Data Cleaning

```{r Setting environment and loading data}
setwd("/Users/eduardaespindola/Documents/Mestrado/W203 - Stats/Lab3/w203-lab3")
crime_data <- read.csv("crime_v2.csv")
```

```{r Overview of the structure of the dataset}
head(crime_data)
str(crime_data)
summary(crime_data)
```

Understanding the meaning of some of the variables, we are able to do some cross checks, and make sure all the data makes sense:

1. County (county): It is the county identifier, and as for the problem statement, we should have only one entry (one row) per county:

```{r Checking for NA}
crime_data[which(is.na(crime_data$county)),]
```

We have no data in these 6 rows, so for the purpose of our analysis, we can get it out

```{r Taking of empty rows}
crime_data<-crime_data[which(!is.na(crime_data$county)),]
```

Now, we must finally check for duplicate values:

```{r Checking duplicate counties}
crime_data[duplicated(crime_data),]
```

We have seen that we have two entries for county 193. The data structure we have should be one row for one county, which is why we are going to discard the extra entry for county 193

```{r Keeping unique values}
crime_data<-unique(crime_data)
```

If we check again for duplicates, it shows us none:

```{r Checking for duplicates yet again}
crime_data[duplicated(crime_data),]
```

2. Year (year): we have that all the observations come from the year of 1987, therefore, we should just check if there are other years on this dataset

```{r Checking years}
summary(crime_data$year)
```

And there we have it, only observations for 1987.

3. Crime Rate (crmrte): It is calculated as ratio of number of reported crimes to the total population of the county. Theoretically, we could have values ranging from zero (no crimes commited in that county in 1987) to infinity (so many crimes committed that the ratio goes to infinity), however both these cases are extremes that don't make any logical sense. So we should check the distribution of this variable to try and spot weird observations:

```{r Checking the Crime Rate}
hist(x = crime_data$crmrte, main = "Crime Rate Distribution", xlab = "Crime Rate", ylab = "Frequency")
summary(crime_data$crmrte)
```

There is nothing abnormal with the data, so it is safe to proceed.

4. Probability of arres (prbarr): The probability of arrest is proxied by the ratio of arrests to offenses.

```{r Checking the Probability of Arrest}
hist(x = crime_data$prbarr, breaks=20, main = "Probability of Arrest Distribution", xlab = "Probability of Arrest", ylab = "Frequency")
summary(crime_data$prbarr)
```

Probabilities should not be over 100%, so we should take a closer look at the observations where the probability of arrest were higher than 1

```{r Checking Probability of Arrest Higher than 1}
crime_data[crime_data$prbarr>1,]
```

For county 115, another thing jumps to the eye, the probability of conviction (prbpris, proxied by the ratio of convictions to arrests), is also higher than 1. Probabilities should range from 0 to 1, however, these anomalies might be due to the way those variables were proxied: probability of arrest is proxied by the ratio of arrests to offenses and the probability of conviction, by the ratio of convictions to arrests. They are not actual probabilities. One may argue that it makes no sense to have more arrests than offenses, or more convictions than arrests, however, we are looking at snapshot of 1987, and arrests made in that year might be referring both to offenses mad in 1987 and previously, which could explain the ration being over than one. The same line of thought applies for the probability of conviction variable: the convictions made in 1987 might be referring both to arrests made in 1987 and previously. For those reasons, we choose not to discard this observation.

5. Probability of Conviction (prbconv): As we have seen previously, the probability of conviction is proxied by the ratio of convictions to arrests.

```{r Probability of Conviction}
summary(crime_data$prbconv)
```

The probability of conviction has some weird values, such one that is empty and another one that is `. We should take a look at those observations

```{r Weird values for probability of conviction}
crime_data$prbconv
crime_data[crime_data$prbconv == '' | crime_data$prbconv=='`',]
```

The observations with these weird values have already been discarded on previous analysis, however, they still show up as factors, since they were first loaded like that. One way we could go is transforming that variable into a numeric one

```{r Converting probability of conviction to numeric}
crime_data$prbconv<-as.numeric(as.character(crime_data$prbconv))
```

Now we can perform the usual analysis:

```{r Checking the Probability of Conviction}
hist(x = crime_data$prbconv, breaks=20, main = "Probability of Conviction Distribution", xlab = "Probability of Conviction", ylab = "Frequency")
summary(crime_data$prbconv)
```

Again, we see observations in which the probability of conviction is higher than 1, which shouldn't happen, if they were in fact probabilities. However, as we previously stated, by the method they were proxied, values above 1 are possible. But, nonetheless, we must analyze those cases in more detail.

```{r Checking cases where the probability of conviction is higher than 1}
crime_data[crime_data$prbconv>1,]
```

Those observations fall into the same issue we have seen for the probability of arrest variable. By the way they were proxied, the ratio of convictions to arrests in 1987 doesn't necessarily matches convictions in 1987 referring to arrests only made in 1987. There might be some convictions made in 1987 referring to arrests made in previous years in the mix, which is why we decide to keep those observations, as the same effect migh also be present in the observations where the probability of conviction was below 1.

6. Probability of Prison Sentence (prbpris): The probability of prison sentence is proxied
by the convictions resulting in a prison sentence to total convictions. In that case, unlike the other two previous variables we analyzed, the ratio is calculated in the same set of convictions: how many of such set of convictions resulted in a prison sentence. Therefore, for this variable, we should have the values ranging from 0 to a maximum of 1.

```{r Checking the Probability of Prison Sentence}
hist(x = crime_data$prbpris, breaks=20, main = "Probability of Prison Sentence Distribution", xlab = "Probability of Prison Sentence", ylab = "Frequency")
summary(crime_data$prbpris)
```

The variable behaves as we expected, and we can move on to analyzing other variables.

7. Average Sentence, days (avgsen): The average sentence time in days. This variable doesn't have a theoretical limit, it only shouldn't be negative. So we just need to be wary of outliers and understand if the values are actually true or some sort of measurement mistake.

```{r Checking the Average Sentence}
hist(x = crime_data$avgsen, breaks=20, main = "Average Sentence Distribution", xlab = "Average Sentence", ylab = "Frequency")
summary(crime_data$avgsen)
```

The variable behaves as we expected, and we can move on to analyzing other variables.

8. Police per Capita (polpc): The ratio of the number of police officers to the total population of the county. The values must be in the range from 0 (no cops in the county) to 1 (everyone in the county is a cop).

```{r Checking the Police per Capita}
hist(x = crime_data$polpc, breaks=20, main = "Police per Capita Distribution", xlab = "Police per Capita", ylab = "Frequency")
summary(crime_data$polpc)
```

The variable behaves as we expected, and we can move on to analyzing other variables.

9. Density (density): People per square mile. This variable should be above zero. Other than that, we should only take a deeper look at outliers.

```{r Checking the Density}
hist(x = crime_data$density, breaks=20, main = "Density Distribution", xlab = "Density", ylab = "Frequency")
summary(crime_data$density)
```

There is a strangely small value for the minimum density, so we should take a deeper look:

```{r Small Density Value}
crime_data[crime_data$density<0.0001,]
```
Searching for the FIPS code of this county (173), we see that it is Swain County. That is clearly an arithmetic error, and the true density value is 0.02. So we must correct it

```{r Correcting Swain County Density}

crime_data$density[crime_data$density<0.0001]<- crime_data$density[crime_data$density<0.0001]*1000
crime_data[crime_data$county==173,]
```


10.Tax Revenue per Capita (taxpc): This variable should be above zero. Other than that, we should only take a deeper look at outliers.

```{r Checking Tax Revenue per Capita}
hist(x = crime_data$taxpc, breaks=20, main = "Tax per Capita Distribution", xlab = "Tax per Capita", ylab = "Frequency")
summary(crime_data$taxpc)
```

The observation in which tax per capita is almost 120 catches the eye, and so we should take a deeper look at that one.

```{r Tax per capita almost 120}
crime_data[crime_data$taxpc>100,]
```

The other variables seem to be ok, so, it is safe to keep these observation.

11. West (west) / 12. Central (central) / 13. Urban (urban): Binary variables that indicate if the county is on West North Carolina, Central North Carolina or in SMSA. All of them should be either 0 or 1 for each observation.

```{r Checking West, Central, Urban}
hist(x=crime_data$west, main = "West", xlab= "West", ylab= "Frequency")
hist(x=crime_data$central, main = "Central", xlab= "Central", ylab= "Frequency")
hist(x=crime_data$urban, main = "Urban", xlab= "Urban", ylab= "Frequency")
```

The variables behave as we expected, and we can move on to analyzing other variables.

14. Percent Minority, 1980 (pctmin80): Percentage of population within minority groups in the year of 1980. It should be between 0 and 1, because it represents the fraction of the population that is within minority groups

```{r Checking Percent Minority}
hist(x = crime_data$pctmin80, breaks=20, main = "Percent Minority Distribution", xlab = "Percent Minority", ylab = "Frequency")
summary(crime_data$pctmin80)
```

The variable behaves as we expected, and we can move on to analyzing other variables.

15. Weekly Wage, Contruction (wcon) / 16. Weekly Wage, Transportation, Utilities and Community (wtuc) / 17. Weekly Wage, Wholesale and Retail Trade (wtrd) / 18. Weekly Wage, Financial, Insurance and Real Estate (wfir) / 19. Weekly Wage, Service Industry (wser) / 20. Weekly Wage, Manufacturing (wmfg) / 21. Weekly Wage, Federal Employees (wfed) / 22. Weekly Wage, State Employees (wsta) / 23. Weekly Wage, Local Government Employees (wloc): All of these variables refer to the average weekly wage in different sectors of the economy. We should check for outliers, and if they do happen, investigate them more deeply.

```{r Checking Wages}
hist(x = crime_data$wcon, breaks=20, main = "Weekly Contruction Wage Distribution", xlab = "Weekly Contruction Wage", ylab = "Frequency")
summary(crime_data$wcon)

hist(x = crime_data$wtuc, breaks=20, main = "Weekly Transportation, Utilities and Community Wage Distribution", xlab = "Weekly Transportation, Utilities and Community Wage", ylab = "Frequency")
summary(crime_data$wtuc)

hist(x = crime_data$wtrd, breaks=20, main = "Weekly Wholesale and Retail Trade Wage Distribution", xlab = "Weekly Wholesale and Retail Trade Wage", ylab = "Frequency")
summary(crime_data$wtrd)

hist(x = crime_data$wfir, breaks=20, main = "Weekly Financial, Insurance and Real Estate Wage Distribution", xlab = "Weekly Financial, Insurance and Real Estate Wage", ylab = "Frequency")
summary(crime_data$wfir)

hist(x = crime_data$wser, breaks=20, main = "Weekly Service Industry Wage Distribution", xlab = "Weekly Service Industry Wage", ylab = "Frequency")
summary(crime_data$wser)

hist(x = crime_data$wmfg, breaks=20, main = "Weekly Manufacturing Wage Distribution", xlab = "Weekly Manufacturing Wage", ylab = "Frequency")
summary(crime_data$wmfg)

hist(x = crime_data$wfed, breaks=20, main = "Weekly Federal Employees Wage Distribution", xlab = "Weekly Federal Employees Wage", ylab = "Frequency")
summary(crime_data$wfed)

hist(x = crime_data$wsta, breaks=20, main = "Weekly State Employees Wage Distribution", xlab = "Weekly State Employees Wage", ylab = "Frequency")
summary(crime_data$wsta)

hist(x = crime_data$wloc, breaks=20, main = "Weekly Local Government Employees Wage Distribution", xlab = "Weekly Local Government Employees Wage", ylab = "Frequency")
summary(crime_data$wloc)
```

For the service industry, there is one observation in particular that catches the eye, which is way above the second largest value. For that, we take a deeper look

```{r Service Industry Investigation}
crime_data[crime_data$wser>2000,]
```

It is county 185, Warren County. The only sector that has a weekly wage so much higher than for the other counties is the service industry, with all other sectors having a weekly wage very close to the state average. One might wonder if this county is particularly attractive for tourism, or some other sort of services, to explain such a difference. That is not the fact: Warren county is a center of tobacco and cotton plantations,educational later textile mills (https://en.wikipedia.org/wiki/Warren_County,_North_Carolina). It is very likely a dot was misplaced, and the actual value is 217.7068 instead of 2177.068. However, since we cannot atest that with certainty, we will leave the value as it is, and will not discard the observation.

24. Offense mix, face-to-face / other (mix): Represents the ratio of criminal offenses made face-to-face (such as armed robbery) to other types. The values can range within any positive number, however, we should dig deeper in the case of outliers.

```{r Checking Mix}
hist(x = crime_data$mix, breaks=20, main = "Offense Mix Distribution", xlab = "Offense Mix", ylab = "Frequency")
summary(crime_data$mix)
```

The variable behaves as we expected, and we can move on to analyzing other variables.

25. Percent Young Male (pctymle): Represents the percent of the population composed by males between the age of 15 and 24. Should be a number between 0 and 1.

```{r Checking Percent Young Male}
hist(x = crime_data$pctymle, breaks=20, main = "Percent Young Male Distribution", xlab = "Percent Young Male", ylab = "Frequency")
summary(crime_data$pctymle)
```

The variable behaves as expected and now we can finally move on to the research question.

## Research Question

As previously stated in our introduction, we are mainly focused on generating actionable insights for reducing crime in a shorter term, therefore, our research question will be focused on the effect variables that are easier to adjust in a smaller time range.

Does a tougher criminal justice system leads to a reduction in crime rates?

For the variables we have, a "tougher criminal justice system" means:

- Higher Sentence Times (avgsen)
- More Police Offices per Capita (polpc)

Those are the variables our team assessed to be of easier action towards change. Policy makers can act upon proposing higher sentence times for crimes, and also hiring more police officers. 

Our output variable will be the Crime Rate (crmrte)


## Model Building

### Model 1: Only with the key explanatory variables:

#### EDA

To build our first model, we must investigate our key explanatory variables, and if needed, propose transformations towards our model building.

##### The output variable: Crime Rate (crmrte)

Let's take a look at how the Crime Rate Variable is distributed:

```{r Crime Rate Exploration}
hist(x=crime_data$crmrte, main = "Crime Rate Distribution", xlab = "Crime Rate", ylab = "Frequency", breaks=20)

summary(crime_data$crmrte)
```

We have a slightly left skewed distribution, however, with the number of observations we have (90 after data cleaning), it is safe to call upon the central limit theorem and assume normality.

##### Key Variables - Average Sentence Time (avgsen):

```{r Average Sentence Time Exploration}
hist(x=crime_data$avgsen, main="Average Sentence Time Distribution", xlab= "Average Sentence Time", ylab="Frequency", breaks=20)
summary(crime_data$avgsen)
```

For this variable we also have a slight left skew, however, the same line of thought we had for crime rate is applicable to average sentence time: the number of observations allows us to call upon the central limit theorem and assume normality.

##### Key Variables - Police per Capita (polpc):

```{r Police per Capita Exploration}
hist(x=crime_data$polpc, main="Police per Capita Distribution", xlab= "Police per Capita", ylab="Frequency", breaks=20)
summary(crime_data$polpc)
```

We have a considerate left skew. In this case, the number of observations is not enough to make us comfortable in calling upon the central limit theorem, and therefore, we choose to try some variable transformations.

###### Log Transformation

```{r Log of Police per Capita Exploration}
hist(x=log(crime_data$polpc), main="Log of Police per Capita Distribution", xlab= "Log Police per Capita", ylab="Frequency", breaks=20)
summary(log(crime_data$polpc))
```

This transformation gives us a distribution much closer to normality, and therefore, we will use the transformed variable log(crime_date$polpc) in our model building.

```{r Creating the logpolpc variable}
crime_data$logpolpc<-log(crime_data$polpc)
```

##### Displaying linear relations

```{r Scatterplots Key Explanatory Variables}

plot(x = crime_data$avgsen, y=crime_data$crmrte, main = "Average Sentence vs Crime Rate", xlab="Average Sentence", ylab="Crime Rate")

plot(x = crime_data$polpc, y=crime_data$crmrte, main = "Police per Capita vs Crime Rate", xlab="Police per Capita", ylab="Crime Rate")

plot(x = crime_data$logpolpc, y=crime_data$crmrte, main = "Log of Police per Capita vs Crime Rate", xlab="Lof of Police per Capita", ylab="Crime Rate")
```

#### Model Creation

As we have explained in our EDA, we will model the crime rate as a function of Average Sentence Time, and our transformed variable for Police per Capita, Log of Police per Capita (logpolpc). Let us first create the model:

```{r Creating model nr 1}
model1<-lm(crmrte ~ avgsen + logpolpc, data = crime_data)
summary(model1)
```

By the coefficients generated by our model, we can analyze them as follows:
- By each day added to the average sentence time, there is an impact of reducing the crimes per person (the crime rate) in 0.0012378. In a county which has a population of 10,000 people, that would mean minus 12 crime offenses.
- By every cop added to the police force of the county, considering the current number of cops as n, the crime rate would be increased by (log(n+1)-log(n))*0.0237214

That poses us with a surprising result: even though increasing sentence time seems to actually impact crime rates negatively, the increase in the number of cops will lead to higher crime rates, though the increase in crime rate will be smaller with each cop added, that is a surprising finding, contrary to what we believed to be true when posing our research question, however, it is perfectly consistent to what we have observed in the scatterplots.

### Model 2: Adding covariates to the mix

In this other part of the model, we are adding some more explanatory variables to our model. Because of our surprising discover regarding the apparent relation between police per capita and the crime rate, we are interested in understanding what could impact police force effectiveness towards crime. We have the data for the average weekly wage in each county for government workers, in the three spheres of power: federal, state and local. Therefore, since police officers are government workers, we are adding these three variables to the mix, as proxies for police force income, in order to understand whether or not better paid police might result in better crime rates.

#### EDA

We are adding three new variables to the mix: Weekly Wage for Federal Employees, Weekly Wage for State Employees, and Weekly Wage for Local Government Employees. So we will explore the relation these variables share with our key explanatory variables and with our outcome variable.

##### Weekly Wage for Federal Employees (wfed)

```{r Exploring wfed}
hist(x=crime_data$wfed, main= "Weekly Wage for Federal Employees Distribution", xlab = "Weekly Wage for Federal Employees", ylab="Frequency", breaks=20)
plot(x=crime_data$wfed, y=crime_data$avgsen, xlab="Weekly Wage for Federal Employees", ylab="Average Sentence", main = "Weekly Wage for Federal Employees vs Average Sentence")
plot(x=crime_data$wfed, y=crime_data$logpolpc, xlab="Weekly Wage for Federal Employees", ylab="Log of Police per Capita", main = "Weekly Wage for Federal Employees vs Log of Police per Capita")
plot(x=crime_data$wfed, y=crime_data$crmrte, xlab="Weekly Wage for Federal Employees", ylab="Crime Rate", main = "Weekly Wage for Federal Employees vs Crime Rate")
```

The variable in question has a fairly normal distribution, and by the scatterplots, we observe that it seems to have no relation to average sentence time, however, positevely correlated to the log of police per capita and with the crime rate, which seems to go in the other direction of what we initially thought.


```{r Exploring wsta}
hist(x=crime_data$wsta, main= "Weekly Wage for State Employees Distribution", xlab = "Weekly Wage for State Employees", ylab="Frequency", breaks=20)
plot(x=crime_data$wsta, y=crime_data$avgsen, xlab="Weekly Wage for State Employees", ylab="Average Sentence", main = "Weekly Wage for State Employees vs Average Sentence")
plot(x=crime_data$wsta, y=crime_data$logpolpc, xlab="Weekly Wage for State Employees", ylab="Log of Police per Capita", main = "Weekly Wage for State Employees vs Log of Police per Capita")
plot(x=crime_data$wsta, y=crime_data$crmrte, xlab="Weekly Wage for State Employees", ylab="Crime Rate", main = "Weekly Wage for State Employees vs Crime Rate")
```

The weekly wage for state employees has a slight left skewed distribution, however, it is safe to assume normality on basis of the central limit theorem. As for its relation with the key explanatory variables and with the output variable, there seems to be none, however, we might uncover something interesting while building our model.

```{r Exploring wloc}
hist(x=crime_data$wloc, main= "Weekly Wage for Local Government Employees Distribution", xlab = "Weekly Wage for Local Government Employees", ylab="Frequency", breaks=20)
plot(x=crime_data$wloc, y=crime_data$avgsen, xlab="Weekly Wage for Local Government Employees", ylab="Average Sentence", main = "Weekly Wage for Local Government Employees vs Average Sentence")
plot(x=crime_data$wloc, y=crime_data$logpolpc, xlab="Weekly Wage for Local Government Employees", ylab="Log of Police per Capita", main = "Weekly Wage for Local Government Employees vs Log of Police per Capita")
plot(x=crime_data$wloc, y=crime_data$crmrte, xlab="Weekly Wage for Local Government Employees", ylab="Crime Rate", main = "Weekly Wage for State Employees vs Crime Rate")
```

The weekly wage for local government employees has a fairly normal distribution, and by the looks of the scatter plots, it doesn't have any clear relation with our key explanatory variables, however, it seems as it has a positive correlation with our output variable, crime rate, contradicting our initial belief that better paid police force would imply in lower crime rates.

#### Model Creation

We are simply adding three more variables to our model (wfed, wsta, wloc), since our EDA didn't show any need for transformation on these variables

```{r}
model2<-lm(crmrte ~ avgsen + logpolpc + wfed + wsta + wloc, data = crime_data)
summary(model2)
```

By the coefficients generated by our model, we can analyze them as follows:
- By each day added to the average sentence time, there is an impact of reducing the crimes per person (the crime rate) in 0.001355. In a county which has a population of 10,000 people, that would mean minus 14 crime offenses.
- By every cop added to the police force of the county, considering the current number of cops as n, the crime rate would be increased by (log(n+1)-log(n))*0.01672.
- By every dollar added to the weekly wage of federal employees, the crime rate would be increased by 0.0001167. In a county with a population of 10,000 people, that would mean 2 more crimes.
- By every dollar added to the weekly wage of state employees, the crime rate would be increased by 0.00005030. In a county with a population of 20,000 people, that would mean an increase of 1 crime.
- By every dollar added to the weekly wage of local government employees, the crime rate would be increased by 0.00003220. In a county with a population of 30,000 people, that would mean an increase of 1 crime.

That is also a surprise in terms of our inital beliefs: higher wages for cops not only don't decrease crime rates, but increase them!

### Model 3: Everything

```{r Model everything}
model3<-lm(crmrte ~ . , data = crime_data)
summary(model3)
```