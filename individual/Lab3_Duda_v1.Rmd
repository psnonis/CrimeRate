---
title: "Lab3"
author: "Eduarda Espindola"
date: "11/25/2018"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Introduction:




## Data Loading and Cleaning


First of all, we must load our data into R:
```{r Loading Data}
crime <- read.csv("../crime_v2.csv", header = T, as.is = T)
```

Now, we have the description for our variables, and how they were calculated / proxied, which will allow us to understand the expected data type and the theoretical range of values.

- space for table with descriptions - 

Now, we can take a look at how our data is structured:
```{r Checking the Data Structure}
head(crime)
```


Now that we have our data loaded, we are going to do some basic sanity checks in out data. We will start by checking for rows with NA values:
```{r Checking for NAs}
crime[is.na(crime$county),]
```

We observe that we have 6 observations with NA's in all of the variables, and therefore, add no information to our analysis. For that reason, we will discard those observations

```{r Removing NAs}
crime<-crime[!is.na(crime$county),]
```

Now that we have removed the NAs, we should check for duplicate observations:

```{r Checking for Duplicates}
crime[duplicated(crime),]
```

We see that county 193 appears twice in our dataset. For that reason, we are discarding one, and therefore, keeping only one observation of county 193 in our dataset.

```{r Removing the Duplicates}
crime<-crime[!duplicated(crime),]
```

Now we go to a deeper analysis. By the description of the variables and how they are calculated, we expect them to be in certain format. Let's check how R loaded the variables.

```{r Checking the types}
str(crime)
```

By the description of the variables, the first thing we notice is that the Probability of Conviction Variable comes as a factor, when it should be a num. We can adjust that:

```{r Converting}
crime$prbconv <- as.numeric(as.character(crime$prbconv))
```

For now, the basic cleaning has been done, however, we should move up to assessing any other oddities with the data, regarding the values it should theoretically assume and also analyzing outliers.

```{r Analyzing outliers}
library(summarytools)
summary(crime[,3:25])
print(dfSummary(crime[,3:25]), method = 'render')
```

Observing the summary for each variable and the summary table for each variable, we notice some rather odd observations. 

_Probabilities above 1_

Theoretically speaking, we should not have probabilities over 1 (100%), but that is what we observe in vairables Probability of Arrest (prbarr) and Probability of Conviction (prbconv). However, when we understand how those variables were proxied, we understand that they are not actual probabilities: they are simply ratios. 

The probability of arrest is proxied by the ratio between the number of arrests in 1987 to the number of offenses in 1987. However, not every arrest made in 1987 might be referring to offenses made in 1987: there might be arrests referring to crimes commited in previous years, which explains why the ratio between arrests and offenses in 1987 could be above 1.

The probability of conviction is proxied by the ratio between the number of convictions and to the number of arrests in 1987. It is the same thing we have observed for the probability of arrest. The convictions issued in 1987 are not all necessarily referring to arrests made in 1987. Besides that, one arrest might lead to several convictions (example, a person arrested might be convicted for several crimes). In that sense, it is possible for us to have this variable achieving values above 1.

_Outliers in other orders of magnitude_

Another thing we observed, both by the histogram and by the summary statistics, is that for the density variable, we have a value that is in a much lower order of magnitude than other observations, with a density of 0.00002. For that, we decide to dig deeper.

```{r Checking the Density}
hist(x = crime$density, breaks=50, main = "Density Distribution", xlab = "Density", ylab = "Frequency")
summary(crime$density)
```

```{r Small Density Value}
crime[crime$density<0.0001,]
```
Searching for the FIPS code of this county (173), we see that it is Swain County. When we search the data for Swain County in 1987 in the United States Census Bureau database, we see that the density was in fact 0.0202. That is clearly an arithmetic error, generating a value a 1000 times smaller. So, we correct it.

```{r Correcting Swain County Density}

crime$density[crime$density<0.0001]<- crime$density[crime$density<0.0001]*1000
crime[crime$county==173,]
```


_Other clear outliers_

For the service industry wages, there is one observation in particular that catches the eye, which is way above the second largest value. For that, we take a deeper look

```{r Service Industry Investigation}
crime[crime$wser>2000,]
```

It is county 185, Warren County. The only sector that has a weekly wage so much higher than for the other counties is the service industry, with all other sectors having a weekly wage very close to the state average. One might wonder if this county is particularly attractive for tourism, or some other sort of services, to explain such a difference. That is not the fact: Warren county is a center of tobacco and cotton plantations, and textile mills (https://en.wikipedia.org/wiki/Warren_County,_North_Carolina). It is very likely the value is multiplied by 10, and the actual value is 217.7068 instead of 2177.068. However, since we cannot atest that with certainty, we will leave the value as it is, and will not discard the observation.
