---
title: 'Reducing Crime'
subtitle: 'MIDS W203, Fall 2018, Lab 3'
author: 'Duda Espindola, Pri Nonis, Laura Pintos, Payman Roghani'
output:
    prettydoc::html_pretty:
        theme: architect
        highlight: github
        toc: true
        number_sections: true
---
<style>
body {
text-align: justify}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

source('assets/utility.R')

import('prettydoc')
import('knitr')
import('kableExtra')
import('tidyverse')
import('RColorBrewer')
import('ggthemes')
import('stargazer')
import('lmtest')
import('maps')
import('summarytools')
import('formattable')
import('sparkline')
import('cowplot')
import('corrplot')

options(digit=3)
```

# Introduction and Research Question

The primary motivation of this report is to provide causal estimates for determinants of crime in the state of North Carolina. The main aim of our study is to shape the public policy platforms, of the political campaign that has hired our services, in the upcoming election cycle. We strive to provide actionable policy recommendations through motivated data analysis of the research question(s).

Our research focuses on the following specific question: <b>Can crime rates be reduced by a tougher criminal justice system?</b> We explore how certainty of punishment and severity of punishment within the criminal justice system affects crime. As one of the key goals of the political campaign is to reduce the crime rate, the natural choice for the outcome variable of our study is the Crime Rate variable. We seek to explain the variability of this variable using Probability of Arrests, Probability of Conviction, and Probability of Prison.

$$
\begin{aligned}
\textbf{Crime Rate} & \sim \textbf{Probability of Arrest} \\
           & + \textbf{Probability of Conviction} \\
           & + \textbf{Probability of Prison}
\end{aligned}
$$

We will primarily use these three explanatory variables as proxies to measure the effects of the the criminal justice system on crime; and this relationship will be explored in our first OLS model. However, we expect other variables to have significant secondary effects on this relationship and will further explore their impact in our extended second OLS model.

## Policy Hypothesis

The null hypothesis assumes that the outcome Crime Rate variable is not impacted by the three explanatory variables Probability of Arrest, Probability of Conviction, and Probability of Prison. We will test the following hypothesis through our detailed data analysis to justify our ultimate policy recommendation(s).

+ <b>Increasing arrest rates will decrease crime rate</b>
+ <b>Increasing conviction rates will decrease crime rate</b>
+ <b>Increasing prison verdicts will decrease crime rate</b>

Our recommendations will be based on sound statistically significant and practically significant results. We hope to reject the null hypothesis by sufficiently explaining the variability of Crime Rate by our explanatory variables. (causal vs associative ..?)

# Data Loading and Cleaning

The primary data source for our study is **Cornwell and W. Trumball (1994), Estimating the Economic Model of Crime with Panel Data, Review of Economics and Statistics 76, 360-366**. We will use a single cross-section of data for **1987** from the multi-year panel. (The authors used panel data methods and instrumental variables to control for some types of omitted variables)

The dataset is provided for the year **1987** except for the **Percent Minority** (pctmin), which is provided for 1980. (Talk about additional datasets we used and why here?)

## Basic Sanity Checks

The dataset contained several technical defects such (1) as empty rows, (2) a duplicated row, and (3) a typo that prevented a numeric variable from being loaded correctly. These issues were easily corrected as shown below.

```{r loading_and_fix}
crime           <- read.csv('crime_v2.csv') # load the dataset
rownames(crime) <- NULL                     # remove row names

crime           <- na.omit(crime)                          # (1) remove empty rows
crime           <- crime[!duplicated(crime$county),]       # (2) remove duplicated row
crime$prbconv   <- as.numeric(as.character(crime$prbconv)) # (3) fix non-numeric value

dim(crime)
```

From the 100 counties of North Carolina our data-set contain a sample of **90** counties. The map below shows the Crime Rate per Capita for the 90 observations; the 10 counties that are not present in the dataset is shown in gray. The dataset contains **25** variables covering the following aspects.

```{r crime_map, fig.height = 4.5, fig.align = 'center', warning = FALSE}
source('assets/maps.R')
map('crmrte', 'Crime Rate per Capita')
```

The missing counties appear to be geographically diverse and gives no reason to suspect the sample quality in terms of randomness, at least with respect to geographical clustering.

## Deeper Analysis

We had to address various anomalies in the dataset, confirming if the values made sense based on the code key and understanding the outlying data points. We used a custom summary table to gleam a high level overview of the data for further analysis. We sorted the variables into four categories as follows.

```{r summary, warning = FALSE}
crime.summary <- tbl_df(t(sapply(1:25, function(n){
    x <- crime[,n]
    c(  Variable  = colnames(crime)[n], CrimeCOR  = round(cor(crime$crmrte,x),  2),
        Mean      = round( mean(x), 2), Median    = round(median(x),            2),
        Min       = round(  min(x), 2), Max       = round(   max(x),            2),
        SD        = round(   sd(x), 2),
        Spread    = list(x),            Histogram = list(hist(x, plot = F)$counts))
    })))

sTable <- function(tab){
    formattable(tab, list(CrimeCOR = color_bar("powderblue"), SD = color_bar("orange"),
        Spread = function(z){
            sapply(z, function(zz){
                knit(text = sprintf("`r sparkline(c(%s), type='box')`",
                                    paste0(zz, collapse = ",")), quiet = T)})},
        Histogram = function(z){
            sapply(z, function(zz){
                knit(text = sprintf("`r sparkline(c(%s), type='bar')`",
                                    paste0(zz, collapse = ",")), quiet = T)})}
    ))}
```

<b>Variable Categories</b>

+ <b>Identification</b>
```{r var_1, results="asis"}
sTable(crime.summary[c(1,2),]) # Identification
```

+ <b>Crime & Law Enforcement</b>
```{r var_2, results="asis"}
sTable(crime.summary[c(3,4,5,6,7,8,25),]) # Crime & Law Enforcement
```
+ <b>Demographics & Geography</b>
```{r var_3, results="asis"}
sTable(crime.summary[c(9,10,11,12,13,14,25),]) # Demographics & Geography
```
+ <b>Weekly Wages</b>
```{r var4, results="asis"}
sTable(crime.summary[c(15,16,17,18,19,20,21,22,23),]) # Weekly Wages
```

### Probabilities Above 1

Theoretically speaking, we should not have probabilities over 1 (100%), but that is what we observe in variables Probability of Arrest (prbarr) and Probability of Conviction (prbconv). However, when we understand how those variables were proxied, we realize that they are not actual probabilities: they are simply ratios. 

The probability of arrest is proxied by the ratio between the number of arrests in 1987 to the number of offenses in 1987. However, not every arrest made in 1987 might be referring to offenses made in 1987: there might be arrests referring to crimes committed in previous years, which explains why the ratio between arrests and offenses in 1987 could be above 1.

The probability of conviction is proxied by the ratio between the number of convictions and to the number of arrests in 1987. It is the same thing we have observed for the probability of arrest. The convictions issued in 1987 are not all necessarily referring to arrests made in 1987. Besides that, one arrest might lead to several convictions (example, a person arrested might be convicted for several crimes). In that sense, it is possible for us to have this variable achieving values above 1.

### Outliers Off by Order of Magnitude

Another anomaly we observed, both by the histogram and by the summary statistics, is that for the density variable. There was a single value that is several orders of magnitude lower than other all other observations, with a density of 0.00002. We decided to investigate this further by using third party data sources.

```{r density_check}
crime$county[crime$density<0.0001]
```

Searching for the FIPS code of this county (173), we see that it is Swain County. When we search the data for Swain County in 1987 in the United States Census Bureau database, we see that the density was in fact 0.0202. That is clearly an arithmetic error, generating a value a 1000 times smaller. Therefore, we corrected it as shown below.

```{r density_fix}
crime$density[crime$county==173]<- crime$density[crime$county==173] * 1000 # correct arithmetic
crime$density[crime$county==173]                                           # error in swain county
```

### Other Significant Outliers

For the service industry wages, there is one observation in particular that catches the eye, which is significantly above the next largest value.

```{r service_check}
crime$county[crime$wser>2000]
```

It is county 185, Warren County. The only sector that has a weekly wage so much higher than for the other counties is the service industry, with all other sectors having a weekly wage very close to the state average. One might wonder if this county is particularly attractive for tourism, or some other sort of services, to explain such a difference. That is not the fact: Warren county is a center of tobacco and cotton plantations, and textile mills (https://en.wikipedia.org/wiki/Warren_County,_North_Carolina). It is very likely the value is multiplied by 10, and the actual value is 217.7068 instead of 2177.068. However, since we cannot test that with certainty, we will leave the value as it is, and will not discard the observation.

## Correlation of the Variables

```{r correlation, fig.align = 'center'}
crime.numeric     <- crime[, !names(crime) %in% c('county','year','west','central','urban')]
crime.correlation <- round(cor(crime.numeric, use = 'pairwise.complete.obs'), 2 )
#crime.ptest      <- cor.test(crime.numeric)$p

corrplot(crime.correlation, diag = FALSE, order = 'FPC', type = "upper")
```

* The Population Density is strongly correlated with Crime Rate.
* The Urban Counties are strongly correlated with Crime Rate.

## Analysis of the Outcome Variable

```{r transform}
crime$crmrte.log_10k <- log(crime$crmrte) * 10000
```

```{r outcome, fig.align = 'center', warning = FALSE}
pHist <- function(x)
{
    ggplot() + 
        geom_histogram(aes(y=..density..,x=x), bins=20, fill="yellow", colour="black") +
        geom_vline(xintercept=mean(x), linetype="dashed", size=1, colour="blue") +
        stat_function(aes(x=x), fun = dnorm, colour = "red", size = 1,
                      args=list(mean=mean(x), sd=sd(x))) +
        theme_economist()
}

p1 <- pHist(crime$crmrte)
p2 <- pHist(crime$crmrte.log_10k)

plot_grid(p1, p2)
```

# Model Building Process


The outcome variable of our study is (crmrte) which is coded as 'crimes per person'.

- What do you want to measure? Make sure you identify variables that will be relevant to the concerns of the political campaign.

- What covariates help you identify a causal effect?

- What covariates are problematic, either due to multicollinearity, or because they will absorb some of a causal effect you want to measure?

- What transformations should you apply to each variable? This is very important because transformations can reveal linearities in the data, make our results relevant, or help us meet model assumptions.

- Are your choices supported by EDA?


Higher Crime Rate Seems to Indicate Higher Police. I.E. the police presence was enhanced in crime ridden counties.

# Regression Models

## Base Model

Based on the variables selected, our base population model is:

$$
\begin{aligned}
\textbf{log(Crime Rate)} & \sim \textbf{Probability of Arrest} \\
           & + \textbf{Probability of Conviction} \\
           & + \textbf{Probability of Prison} \\
           & + \textbf{u}
\end{aligned}
$$

```{r base}
model.1 <- with(crime, lm(log(crmrte)~prbarr+prbconv+prbpris))
coefs.1 <- coef(model.1)
```

<b>Model Coefficients</b>

* $\beta_0$ : The **`r names(coefs.1[1])`** is `r round(coefs.1[1],3)`, which can not be interpreted in a meaningful way without considering the other coefficients.
* $\beta_1$ : The **`r names(coefs.1[2])`** coefficient is `r round(coefs.1[2],3)`, which means we could predict that for every 1% increase in Probability of Arrest that Crime Rate will go down by approximately 1.99%, while holding all other covariates and unobserved factors fixed.
* $\beta_2$ : The **`r names(coefs.1[3])`** coefficient is `r round(coefs.1[3],3)`, which means we could predict that for every 1% increase in Probability of Conviction that Crime Rate will go down by approximately 0.74%, while holding all other covariates and unobserved factors fixed.
* $\beta_3$ : The **`r names(coefs.1[4])`** coefficient is `r round(coefs.1[4],3)`, which means we could predict that for every 1% increase in Probability of Prison that Crime Rate will go **up** by approximately 0.34%, while holding all other covariates and unobserved factors fixed.

<b>Goodness of Fit</b>

<b>6 CLM Assumptions</b>

## Second Model

$$
\begin{aligned}
\textbf{log(Crime Rate)} & \sim \textbf{Probability of Arrest} \\
           & + \textbf{Probability of Conviction} \\
           & + \textbf{Probability of Prison} \\
           & + \textbf{u}
\end{aligned}
$$

```{r second}
model.2 <- with(crime, lm(log(crmrte)~prbarr+prbconv+prbpris))
```

## Third Model
## Regression Table

# Ommitted Variables

Possible explanatory variables that are not available :
- Level of education.
- Unemployment.
- Private security.
- Severity of crimes; misdemeanors vs felonies.
- Type of crimes; drug offenses vs violent crimes, or white collar crimes vs blue collar crimes etc.

# Conclusion

- Does the conclusion address the high-level concerns of a political campaign?
- Is the discussion connected to whether the key effects are real or whether they may be solely an artifact of omitted variable bias?

