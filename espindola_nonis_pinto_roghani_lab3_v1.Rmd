---
title: 'Reducing Crime'
subtitle: 'MIDS W203, Fall 2018, Lab 3'
author: 'Duda Espindola, Pri Nonis, Laura Pintos, Payman Roghani'
output:
    prettydoc::html_pretty:
        theme: architect
        highlight: github
        toc: true
        number_sections: true
---
<style>
body {
text-align: justify}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

source('assets/utility.R')

import('knitr')
import('kableExtra')
import('tidyverse')
import('RColorBrewer')
import('ggthemes')
import('stargazer')
import('lmtest')
import('maps')
import('formattable')
import('sparkline')
import('cowplot')
import('corrplot')
import('ggfortify')
import('GGally')

options(digit=3)
```

# Introduction and Research Question

The primary motivation of this report is to provide causal estimates for determinants of crime in the state of North Carolina. The main aim of our study is to shape the public policy platforms, of the political campaign that has hired our services, in the upcoming election cycle. We strive to provide actionable policy recommendations through motivated data analysis of the research question(s).

Our research focuses on the following specific question: <b>Can crime rates be reduced by a tougher criminal justice system?</b> We explore how certainty of punishment and severity of punishment within the criminal justice system affects crime. As one of the key goals of the political campaign is to reduce the crime rate, the natural choice for the outcome variable of our study is the Crime Rate variable. We seek to explain the variability of this variable using Probability of Arrests, Probability of Conviction, and Probability of Prison.

$$
\begin{aligned}
\textbf{Crime Rate} & \sim \textbf{Probability of Arrest} \\
           & + \textbf{Probability of Conviction} \\
           & + \textbf{Probability of Prison}
\end{aligned}
$$

We will primarily use these three explanatory variables as proxies to measure the effects of the the criminal justice system on crime; and this relationship will be explored in our first OLS model. However, we expect other variables to have significant secondary effects on this relationship and will further explore their impact in our extended second OLS model.

## Policy Hypothesis

The null hypothesis assumes that the outcome Crime Rate variable is not impacted by the three explanatory variables Probability of Arrest, Probability of Conviction, and Probability of Prison. We will test the following hypothesis through our detailed data analysis to justify our ultimate policy recommendation(s).

+ <b>Increasing arrest rates will decrease crime rate</b>
+ <b>Increasing conviction rates will decrease crime rate</b>
+ <b>Increasing prison verdicts will decrease crime rate</b>

Our recommendations will be based on sound statistically significant and practically significant results. We hope to reject the null hypothesis by sufficiently explaining the variability of Crime Rate by our explanatory variables. (causal vs associative ..?)

# Data Loading and Cleaning

The primary data source for our study is **Cornwell and W. Trumball (1994), Estimating the Economic Model of Crime with Panel Data, Review of Economics and Statistics 76, 360-366**. We will use a single cross-section of data for **1987** from the multi-year panel. (The authors used panel data methods and instrumental variables to control for some types of omitted variables)

The dataset is provided for the year **1987** except for the **Percent Minority** (pctmin), which is provided for 1980. (Talk about additional datasets we used and why here?)

## Basic Sanity Checks

The dataset contained several technical defects such (1) as empty rows, (2) a duplicated row, and (3) a typo that prevented a numeric variable from being loaded correctly. These issues were easily corrected as shown below.

```{r loading_and_fix}
crime           <- read.csv('crime_v2.csv') # load the dataset
rownames(crime) <- NULL                     # remove row names

crime           <- na.omit(crime)                          # (1) remove empty rows
crime           <- crime[!duplicated(crime$county),]       # (2) remove duplicated row
crime$prbconv   <- as.numeric(as.character(crime$prbconv)) # (3) fix non-numeric value

dim(crime)
```

From the 100 counties of North Carolina our data-set contain a sample of **90** counties. The map below shows the Crime Rate per Capita for the 90 observations; the 10 counties that are not present in the dataset is shown in gray. The dataset contains **25** variables covering the following aspects.

```{r crime_map, fig.height = 4.5, fig.align = 'center', warning = FALSE}
source('assets/maps.R')
map('crmrte', 'Crime Rate per Capita')
```

The missing counties appear to be geographically diverse and gives no reason to suspect the sample quality in terms of randomness, at least with respect to geographical clustering.

## Deeper Analysis

We had to address various anomalies in the dataset, confirming if the values made sense based on the code key and understanding the outlying data points. We used a custom summary table to gleam a high level overview of the data for further analysis. We sorted the variables into four categories as follows.

```{r summary, warning = FALSE}
crime.summary <- tbl_df(t(sapply(1:25, function(n){
    x <- crime[,n]
    c(  Variable  = colnames(crime)[n], CrimeCOR  = round(cor(crime$crmrte,x),  2),
        Mean      = round( mean(x), 2), Median    = round(median(x),            2),
        Min       = round(  min(x), 2), Max       = round(   max(x),            2),
        SD        = round(   sd(x), 2),
        Spread    = list(x),            Histogram = list(hist(x, plot = F)$counts))
    })))

sTable <- function(tab){
    formattable(tab, list(CrimeCOR = color_bar("powderblue"), SD = color_bar("orange"),
        Spread = function(z){
            sapply(z, function(zz){
                knit(text = sprintf("`r sparkline(c(%s), type='box')`",
                                    paste0(zz, collapse = ",")), quiet = T)})},
        Histogram = function(z){
            sapply(z, function(zz){
                knit(text = sprintf("`r sparkline(c(%s), type='bar')`",
                                    paste0(zz, collapse = ",")), quiet = T)})}
    ))}
```

<b>Variable Categories</b>

+ <b>Identification</b>
```{r var_1, results="asis"}
sTable(crime.summary[c(1,2),]) # Identification
```

+ <b>Crime & Law Enforcement</b>
```{r var_2, results="asis"}
sTable(crime.summary[c(3,4,5,6,7,8,25),]) # Crime & Law Enforcement
```
+ <b>Demographics & Geography</b>
```{r var_3, results="asis"}
sTable(crime.summary[c(9,10,11,12,13,14,25),]) # Demographics & Geography
```
+ <b>Weekly Wages</b>
```{r var4, results="asis"}
sTable(crime.summary[c(15,16,17,18,19,20,21,22,23),]) # Weekly Wages
```

### Probabilities Above 1

Theoretically speaking, we should not have probabilities over 1 (100%), but that is what we observe in variables Probability of Arrest (prbarr) and Probability of Conviction (prbconv). However, when we understand how those variables were proxied, we realize that they are not actual probabilities: they are simply ratios. 

The probability of arrest is proxied by the ratio between the number of arrests in 1987 to the number of offenses in 1987. However, not every arrest made in 1987 might be referring to offenses made in 1987: there might be arrests referring to crimes committed in previous years, which explains why the ratio between arrests and offenses in 1987 could be above 1.

The probability of conviction is proxied by the ratio between the number of convictions and to the number of arrests in 1987. It is the same thing we have observed for the probability of arrest. The convictions issued in 1987 are not all necessarily referring to arrests made in 1987. Besides that, one arrest might lead to several convictions (example, a person arrested might be convicted for several crimes). In that sense, it is possible for us to have this variable achieving values above 1.

### Outliers Off by Order of Magnitude

Another anomaly we observed, both by the histogram and by the summary statistics, is that for the density variable. There was a single value that is several orders of magnitude lower than other all other observations, with a density of 0.00002. We decided to investigate this further by using third party data sources.

```{r density_check}
crime$county[crime$density<0.0001]
```

Searching for the FIPS code of this county (173), we see that it is Swain County. When we search the data for Swain County in 1987 in the United States Census Bureau database, we see that the density was in fact 0.0202. That is clearly an arithmetic error, generating a value a 1000 times smaller. Therefore, we corrected it as shown below.

```{r density_fix}
crime$density[crime$county==173]<- crime$density[crime$county==173] * 1000 # correct arithmetic
crime$density[crime$county==173]                                           # error in swain county
```

### Other Significant Outliers

For the service industry wages, there is one observation in particular that catches the eye, which is significantly above the next largest value.

```{r service_check}
crime$county[crime$wser>2000]
```

It is county 185, Warren County. The only sector that has a weekly wage so much higher than for the other counties is the service industry, with all other sectors having a weekly wage very close to the state average. One might wonder if this county is particularly attractive for tourism, or some other sort of services, to explain such a difference. That is not the fact: Warren county is a center of tobacco and cotton plantations, and textile mills (https://en.wikipedia.org/wiki/Warren_County,_North_Carolina). It is very likely the value is multiplied by 10, and the actual value is 217.7068 instead of 2177.068. However, since we cannot test that with certainty, we will leave the value as it is, and will not discard the observation.

## Correlation of the Variables

```{r correlation, fig.align = 'center'}
crime.numeric     <- crime[, !names(crime) %in% c('county','year','west','central','urban')]
crime.correlation <- round(cor(crime.numeric, use = 'pairwise.complete.obs'), 2 )
#crime.ptest      <- cor.test(crime.numeric)$p

corrplot(crime.correlation, diag = FALSE, order = 'FPC', type = "upper")
```

* The Population Density is strongly correlated with Crime Rate.
* The Urban Counties are strongly correlated with Crime Rate.

# Model Building Process

> - What do you want to measure? Make sure you identify variables that will be relevant to the concerns of the political campaign.
> - What covariates help you identify a causal effect?
> - What covariates are problematic, either due to multicollinearity, or because they will absorb some of a causal effect you want to measure?
> - What transformations should you apply to each variable? This is very important because transformations can reveal linearities in the data, make our results relevant, or help us meet model assumptions.
> - Are your choices supported by EDA?

The study of the research question requires some exploratory data analysis to choose the correct variables that identify the true relationship we are trying to model. The section below details the process of defining the models.

## Selection of the Outcome Variable

Crime rate is our dependent variable; therefore, we need to conduct additional exploratory data analysis on this variable, in addition to steps taken in our general EDA above.  

In our initial exploration, we noticed that the crime rate histogram showed a positively skewed distribution. A log transformation of the variable pulls the outlying data points closer to the bulk of the observations, resulting in a normal distribution. More importantly, the log transformation will allow us to interpret changes in the dependent variable as percentages, which is a more meaningful way to describe such changes in this context. Counties have different crime rates and percentage changes in crime will enable county-to-county comparison. As a result, we decided to use the log transformation in our model. 

There is an extreme outlier on the left tail of the data, but we decided to keep that as we don’t have a strong reason for removing it.

```{r outcome_plot, fig.align = 'center', warning = FALSE}
p1 <- pHist(    crime$crmrte , breaks = 20, label = "Crime Rate"      )
p2 <- pHist(log(crime$crmrte), breaks = 20, label = "Crime Rate (log)")

plot_grid(p1, p2)
```

```{r outcome_trans}
crime$crmrte.log <- log(crime$crmrte) # log transformation
```

Summary statistics of the transformed variable, doesn’t show any issues.

```{r outcome_summary}
summary(crime$crmrte.log) # assessment of key variables
```

## Explanatory Variables, Base Model

As explained in the introduction, in order to test our hypothesis regarding the impact of a tougher criminal justice system on crime rate, we are using 3 explanatory variables in our base model:

- Probability of Arrest: Defined as the ratio of offenses to arrests. Using this variable, we would like to assess the hypothesis that more stringent arrest protocols and improvements in crime detection would lead to lower crime rate.
- Probability of Conviction: Defined as the ratio of convictions to arrests. If our hypothesis regarding a negative impact of higher convictions/arrests ratio on crime rate is true, then this could lead to highly actionable measures. For instance, stricter sentencing guidelines could implemented, followed by allocation of more resources to law enforcement agencies to collect more effective evidence.
- Probability of Prison: Higher imprisonment rate, as one of the harshest types of criminal sentencing, could have a deterrent effect, leading to lower crime rate. Hence our interest in this variable. 

Looking at the summary statistics of the 3 variables, we don’t see any alarming issues. Also, histograms show a fairly normal distribution for all 3 variables. Although, there is 1 extreme outlier in `prbarr` and one in `prbarr` (which are both from the same observation, county 115) we decided to keep them in our data because we don’t have any reason to believe that they are erroneous values. That said, we will evaluate the influence of these extreme point during or diagnostic analysis of regression models. 

County 115 consistently shows up in the histograms of crime rate, prbarr and prbconv as an extreme outlier. It has a low density (below first quartile), which could be the reason for the unusual values  in our variables of interest.

```{r m1_explanatory_summary}
summary(crime$prbarr ) # assessment of explanatory variables
summary(crime$prbconv) # assessment of explanatory variables
summary(crime$prbpris) # assessment of explanatory variables
```

```{r m1_explanatory_plot, fig.align = 'center', warning = FALSE}
p1 <- pHist(crime$prbarr,  breaks = 40, label = "Probability of Arrest"    )
p2 <- pHist(crime$prbconv, breaks = 40, label = "Probability of Conviction")
p3 <- pHist(crime$prbpris, breaks = 40, label = "Probability of Prison"    )

plot_grid(p1, p2, p3) # TODO : make three columns
```

Next, we looked at the scatterplots of dependent and explanatory variables for our base model. `prbarr`  seems to have a pretty linear relationship with `crmrte.log`. The same with `prbconv`, although we see a curvature towards the right side of the chart. 
`prbpris` does not seem to have a linear relationship with `crmrte.log`, based on the scatterplot; it looks more like a quadratic relationship. However, we decided to leave `prbpris` as is, because a quadratic transformation would make the interpretation of our model unnecessarily complicated. 

```{r m1_correlation, fig.align = "center", warnings = FALSE}
ggscatmat(crime, columns = c(26,4,5,6)) + geom_smooth(method = "lm") + theme_economist()
```

## Explanatory Variables, Extended Model

For Model 2, we decided to add two additional explanatory variables to our model.

- Avg. Sentence, Days (`avgsen`): We believe that longer prison sentences could have a greater deterrent effect in the community, leading to lower crime rate.

- Police per Capita(`polpc`): We chose this covariate based on the assumption that a higher number of cops in charge would mean an unsafe environment for individuals to commit crimes, which in turn builds a safer community. 

Having seen positive skewness in the distributions of both `avgsen` and `polpc` in our in initial EDA, we compared the histograms of these 2 variables in the original form and after log transformation. Since the log transformation results in a distribution closer to normal for both variables, we decided to use log transformations in our model. 

The outiler observed in these histograms are values from county 115. The low density of county 115 could explain why it is an outlier, the average sentence variable could be  very sensitive to violent crimes in a low density county.

```{r m2_explanatory_plot, fig.align = "center", warnings = FALSE}
p1 <- pHist(    crime$polpc  , breaks = 20, label = "Police per Capita"         )
p2 <- pHist(log(crime$polpc) , breaks = 20, label = "Police per Capita (log)"   )
p3 <- pHist(    crime$avgsen , breaks = 20, label = "Avg. Prison Sentence"      )
p4 <- pHist(log(crime$avgsen), breaks = 20, label = "Avg. Prison Sentence (log)")

plot_grid(p1,p2,p3,p4)
```

```{r m2_explanatory_transform}
crime$avgsen.log <- log(crime$avgsen) # log transformation
crime$polpc.log  <- log(crime$polpc ) # log transformation
```

Summary statistics of the transformed variables, don't show any issues.

```{r m2_explanatory_summary}
summary(crime$polpc.log ) # assessment of key variables
summary(crime$avgsen.log) # assessment of key variables
```

We then looked at the scatterplots of dependent and the additional explanatory variables for our second model. None of the additional variables seem to have a perfect linear relationship with the independet variable. But since we have already implemented a log transformation and seen improvment in variable distributions, we will not take additional steps to modify our variables and use them in their current state.

```{r m2_correlation, fig.align = "center", warnings = FALSE}
ggscatmat(crime, columns = c(26,27,28)) + geom_smooth(method = "lm") + theme_economist()
```

## Explanatory Variables, Kitchen-sink Model

Finally, we are adding almost other variables (except for `county` and `year`) to our 3rd model to compare with the other two. The exploratory data analysis for all these variable is included in Data Loading and Cleaning section of this report. 

The only additional step we took for Model 3, was evaluation of variable `mix` due to the positive skewness that we observed in its distribution. Here we are comparing the distribution of `mix` before and after log transformation, and since the distribution post log transformation looks fairly normal, we decided to use it in our Model 3.

```{r m3_explnatory_plot, fig.align = "center", warnings = FALSE}
p1 <- pHist(    crime$mix , breaks = 20, label = "Offense Mix"      )
p2 <- pHist(log(crime$mix), breaks = 20, label = "Offense Mix (log)")

plot_grid(p1, p2)
```

```{r m3_explanatory_transform}
crime$mix.log <- log(crime$mix) # log transformation
```

We don’t see any issue in the summary statistics of the transformed variable `mix.log`.

```{r m3_explanatory_summary}
summary(crime$mix.log)
```

# Regression Models

We would like to address our key research question to understand whether strict criminal laws and their enforement result in lower crime rate. 

## Base Model

Based on the variables selected, our base population model is:

$$
\begin{aligned}
\textbf{log(Crime Rate)} & \sim \textbf{Probability of Arrest} \\
           & + \textbf{Probability of Conviction} \\
           & + \textbf{Probability of Prison} \\
           & + \textbf{u}
\end{aligned}
$$

```{r m1}
m1 <- lm(crmrte.log ~ prbarr + prbconv + prbpris, data = crime)
m1
```

### Coefficients

- $\beta_0$ : `intercept` is -2.6846, which can not be interpreted in a meaningful way without considering other coefficients.
- $\beta_1$ : `prbarr` coefficient is -1.9992, which means we could predict that for every 0.01 increase in probability of arrest, crime rate will go down by approximately 1.99%, while holding all other covariates and unobserved factors fixed.
- $\beta_2$ : `prbconv` coefficient is -0.7364, which means we could predict that for every 0.01 increase in probability of conviction, crime rate will go down by approximately 0.74%, while holding all other covariates and unobserved factors fixed.
- $\beta_3$ : `prbpris` coefficient is 0.388, which means that every 0.01 increase in probability of prison is associated with a 0.39% increase in crime rate, while holding all other covariates and unobserved factors fixed.

It is worth noting that the coefficients for `prbarr` and `prbconv` amplify the effect of increasing `prbarr` or `prbconv`. This is not true for the coefficient of `prbpris`.

### Goodness of Fit

The R-squared of the base model is  0.4505, which means around 45% of the variation in crime rate could be explained by our model
AIC (Akaike Information Criterion): The AIC value for base model is 102.4805, which we will compare with those of the next models to evaluate goodness of fit

```{r m1_summary}
summary(m1)$r.squared
AIC(m1)
```

### 6 CLM Assumptions

1. Linearity: Our model is linear in parameters as shown above
2. IID Sampling: We don’t have sufficient insight into how the data have been collected. For example, we don’t know if the probability of arrest is calculated by dividing the number of all arrests by the number all crimes across counties in 1987, or from a sample. But since the data are collected by key government agencies and used for analysis by reputable researchers, we assume random sampling. Another issue is that we don’t have data from some of counties and we are not sure how including additional data from those counties would affect our analysis. As a result, the insights from our regression modelling might not be applicable for the entire North Carolina, unless we have full insight into the missing data.  An additional concern that we have with the sampling is that the data we have is only for year 1987. This specific year is know for the biggest  crash in the stock market in one single day.  If this event created specific conditions like an abnormal number of crime offenses (i.e. due to poverty, depression, anger, etc.) our sample is biased.
3. Multicollinearity: We didn’t see any sign of perfect collinearity among our explanatory variables. In addition, R would warn users if such collinearity exists, which did not happen throughout our analysis.
4. Zero Conditional Mean: We will cover that below.
5. Homoskedasticity: See below.
6. Normality: See below.

### Residuals Plot

```{r m1_residuals, fig.align = 'center'}
pHist(m1$residuals, breaks = 20, label = "Residuals")
```

The histogram of residuals show a fairly normal distribution where the bulk of the data points are (except for a few spikes that don’t look totally abnormal). However, we could see the extreme outlier on the left that creates some sort of negative skew in the distribution. 

### Model Diagnostics

```{r m1_diagnostics, fig.align = 'center'}
autoplot(m1) + theme_economist()
```

<b>Residual vs Fitted</b>
<br/>
The residual vs fitted spline shows curvatures, deviating from zero, both on the left and the right side. The one on the left is the result of extreme outlier that we observed before. The curvature on the right side, as the `crmrte.log` increases, might be because we don’t have enough data points. Either way, this chart doesn’t provide the confidence to verify the zero mean condition assumption.

<b>Normal Q-Q</b>
<br/>
Most points are on, or fairly close to the diagonal line, based on which we can tell that the residuals are distributed normally. However, we see a little bit of deviation towards the two ends of the line. Thus, we will take additional steps to verify this assumption. 

<b>Scale-Location</b>
<br/>
Despite the fact that points on chart seem to spread out as we move to the right, there is not strong sign of heteroskedasticity, since we have very few points on the left side.

<b>Residuals vs Leverage</b
<br/>
As suspected, the residual from observation row 51 (county 115), has a high influence on our model, with a Cook’s distance of about 1

<b>Breusch-Pagan Test</b>
<br/>
In order to evaluate CLM assumption 5 `homoskedasticity` we used the Breusch-Pagan test.

```{r m1_heteroskedasticity}
bptest(m1) # check for heteroskedasticity
```

The p-value is greater than 0.05, which means our null hypothesis (absence of heteroskedasticity) can not be rejected. In other words there is no heteroskedasticity in our model.

<b>Shapiro-Wilk Test</b>
<br/>
In order to evaluate CLM assumption 6 `normality` we used the Shapiro-Wilk test.

```{r m1_normality}
shapiro.test(m1$residuals) # check for normality
```

The p-value is greater than 0.05, which means we can not reject the null hypothesis (that residuals are drawn from a population with normal distribution). In other words the residuals in our model are normally distributed.

### Interpretation and Conclusion

As evidenced by the coefficients or base model, we can state that increases in probability of arrest and probability of conviction could potentially lower crime rate. In other words, a policy focused on more stringent arrest protocols and stricter criminal sentencing could be proposed by the political campaign. 
The probability of prison has a positive coefficient in our base model, meaning a higher prison to conviction ratio is associated with higher crime rate. We don’t believe this relationship means that, for example, if we increase the probability of prison, the crime rate will go up. We think that this might be due to the fact that the prison to conviction ratio is already high in areas where crime rate is high. For a more effective assessment of such a relationship we need to have data to see the trends in crime rate before and after the prison to conviction rate goes up as a result of policy change, which is not currently included in our dataset.  An additional possible explanation is that incarceration does not deter crime as it exposes the prisoners to an environment which could amplify their criminal behaviour when they finish their sentence. 

## Extended Model

As explained in the Model Building Process section, we are adding 2 other covariates to out second model: Avg. Sentence, Days (`avgsen`) and Police per Capita(`polpc`). Not only these 2 variables could help us provide actionable recommendations to the political campaign, but also, we assumed, they are correlated with the 3 variables in the base model and thus make the model more accurate. 

$$
\begin{aligned}
\textbf{log(Crime Rate)} & \sim \textbf{Probability of Arrest} \\
           & + \textbf{Probability of Conviction} \\
           & + \textbf{Probability of Prison} \\
           & + \textbf{log(Average Sentence)} \\
           & + \textbf{log(Police per Capita)} \\
           & + \textbf{u}
\end{aligned}
$$

```{r m2}
m2 <- lm(crmrte.log ~ prbarr + prbconv + prbpris + avgsen.log + polpc.log, data = crime)
m2$coefficients
```

### Coefficients

While holding all other covariates and ubobserved factors fixed:

- $\beta_0$ : `intercept`  is `r coef(m2)[1]`, which can not be interpreted in a meaningful way without considering the other coefficients
- $\beta_1$ : `prbarr`     is `r coef(m2)[2]`, which means we could predict that for every 0.01 increase in probability of arrest, crime rate will go down by approximately 2.35%
- $\beta_2$ : `prbconv`    is `r coef(m2)[3]`, which means we could predict that for every 0.01 increase in probability of conviction, crime rate will go down by approximately 0.73%
- $\beta_3$ : `prbpris`    is `r coef(m2)[4]`, which means that every 0.01 increase in probability of prison is associated with a 0.39% increase in crime rate
- $\beta_4$ : `polpc.log`  is `r coef(m2)[5]`, which means that every 1% increase in police per capita is associated with a 0.62% increase in crime rate
- $\beta_5$ : `avgsen.log` is `r coef(m2)[6]`, which means we could predict that for every 1% increase in avg. prison sentence, crime rate will go down by approximately 0.065%

### Goodness of Fit

The R-squared of the base model is  0.6101, which means around 61% of the variation in crime rate could be explained by our model. Even though this is a higher number compared to our base model, it doesn’t necessarily show a better fit because when we add variables R-squared goes up anyway. That is the reason we look at the AIC value to compare the 2 models in terms of goodness of fit.
AIC (Akaike Information Criterion): The AIC value for base model is 75.60, which is lower than that of base model (102.5). Therefore, we can say that our model 2 is more accurate than the base model.

```{r m2_summary}
summary(m2)$r.squared
AIC(m2)
```

### 6 CLM Assumptions

1. Linearity: Our model is linear in parameters.
2. IID Sampling: See our note for the base model. 
3. Multicollinearity: We didn’t see any sign of perfect collinearity among our explanatory variables. In addition, R would warn users if such collinearity exists, which did not happen throughout our analysis.
4. Zero Conditional Mean: We will cover that below.
5. Homoskedasticity: See below.
6. Normality: See below.

### Residuals Plot

```{r m2_residuals, fig.align = 'center'}
pHist(m2$residuals, breaks = 20, label = "Residuals")
```

The histogram of residuals show a fairly normal distribution (especially compared with the same plot for base model) where the bulk of the data points are. However, we could see the extreme outlier on the left that creates some sort of negative skew in the distribution. 

### Model Diagnostics

```{r m2_diagnostics, fig.align = 'center'}
autoplot(m2) + theme_economist()
```

<b>Residual vs Fitted</b>
<br/>
The residual vs fitted spline is much flatter than that of base model. However, it shows a curvature with positive slope on the right side, deviating from zero. This might be due to the fact that we don’t have enough data points.

<b>Normal Q-Q</b>
<br/>
Most points are on, or fairly close to the diagonal line. However, as in base model, we see some deviations towards the two extremes of the line. Thus, we will take additional steps to verify this assumption. 

<b>Scale-Location</b>
<br/>
Despite the fact that points on chart seem to spread out as we move to the right, there is not strong sign of heteroskedasticity, since we have very few points on the left side.

<b>Residuals vs Leverage</b>
<br/>
As in the base model, the residual from observation row 51 (county 115), has a high influence on our model. But in model 2, its Cook’s distance is much lower than what we saw in the base model.

<b>Breusch-Pagan Test</b>
<br/>
In order to evaluate CLM assumption 5 `homoskedasticity` we used the Breusch-Pagan test.

```{r m2_heteroskedasticity}
bptest(m2) # check for heteroskedasticity
```

The p-value is greater than 0.05, which means our null hypothesis (absence of heteroskedasticity) can not be rejected. In other words there is no heteroskedasticity in our model.

<b>Shapiro-Wilk Test</b>
<br/>
In order to evaluate CLM assumption 6 `normality` we used the Shapiro-Wilk test.

```{r m2_normality}
shapiro.test(m2$residuals) # check for normality
```

The p-value is smaller than 0.05, which means we can reject the null hypothesis (that residuals are drawn from a population with normal distribution). That said, given the sample size of >30 we can assume that the sampling distribution of our coefficients is normal, so assumption 6 (normality) still holds in our model.

### Interpretation and Conclusion

As shown in the analysis above, our second model confirms what we found in our base model: increases in probability of arrest and probability of conviction could potentially lower crime rate. In other words, a policy focused on more stringent arrest protocols and stricter criminal sentencing could be proposed by the political campaign. Moreover, an increase in avg. prison sentence could lead to lower crime rate as explained above. This is another recommendation we plan to provide to the political campaign, a policy for longer prison time. 

As in the base model, the probability of prison has a positive coefficient in our base model, meaning a higher prison to conviction ratio is associated with higher crime rate. Also, in model 2, we found that increase in police per capita is associated with higher crime rate. We believe that the relationships we observed might be due to the fact that the prison to conviction ratio and police per capita are already high in areas where crime rate is high. As stated before, for a more effective assessment of this situation, we would have to have data to see the trends in crime rate before and after changes in prison/conviction ratio and police per capita where they are implemented. 

## Kitchen-sink Model

For the third model, we are adding to the second model the rest of  the variables in the dataset as predictors, with the exception of year and county.  With this approach we are not following a criteria for inclusion of a variable in the model (i.e. is the variable relevant or not in the context of our research question, does it have a confounding effect or not with the variables in the previous models) and we do not expect to obtain the most parsimonious model that describes our data.  This third model would probably be an overfitting model.

$$
\begin{aligned}
\textbf{log(Crime Rate)} & \sim \textbf{Everything} \\
           & + \textbf{u}
\end{aligned}
$$

```{r m3}
m3 = lm(crmrte.log ~ prbarr + prbconv + prbpris + avgsen.log + polpc.log+
        density+taxpc+west+central+urban+pctmin80+wcon+wtuc+
        wtrd+wfir+wser+wmfg+wfed+wsta+wloc+log(mix)+pctymle, data = crime)

m3 <- lm(crmrte.log ~ prbarr + prbconv + prbpris + polpc.log + avgsen.log + 
                 taxpc + density + west + central + urban + pctmin80 + wcon +
                 wtuc + wtrd + wfir + wser + wmfg + wfed + wsta + wloc +
                 mix.log + pctymle, data = crime)
m3
```

```{r m3_summary}
summary(m3)$r.squared
    AIC(m3)
```

### Coefficients

### Goodness of Fit

### 6 CLM Assumptions

### Residuals Plot

```{r m3_residuals}
pHist(m3$residuals, breaks = 20, label = "Residuals")
```

### Model Diagnostics

```{r m3_diaggnostics, fig.align = 'center'}
autoplot(m3) + theme_economist()
```

```{r m3_normality}
shapiro.test(m3$residuals) # test to check for normality
```
- Shapiro-Wilk test: The p-value > 0.05, which means we can not reject the null hypothesis (residuals are drawn from a population with normal distribution). In other words the residuals in our model are normally distributed.

```{r m3_heteroskedasticity}
bptest(m3) # check for heteroskedasticity
```
- Breusch-Pagan test: The p-value < 0.05, which means our null hyothesis (absence of heteroskedasticity) can be rejected. In other words there is heteroskedasticity in our model.

# Regression Table

```{r regression, results = "asis"}
stargazer(m1, m2, m3,
          type      = "html", 
          report    = "vc",        # Don't report errors
          title     = "Linear Models Crime Rate",
          keep.stat = c("rsq", "n"),
          omit.table.layout = "n") # Omit more output related to errors
```

