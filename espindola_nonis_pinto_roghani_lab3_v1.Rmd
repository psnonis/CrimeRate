---
title: 'Reducing Crime'
subtitle: 'MIDS W203, Fall 2018, Lab 3'
author: 'Duda Espindola, Pri Nonis, Laura Pintos, Payman Roghani'
output:
    prettydoc::html_pretty:
        theme: architect
        highlight: github
        toc: true
        number_sections: true
---
<style>
body {
text-align: justify}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

options(digit=3)

source('assets/utility.R')

import('prettydoc')
import('knitr')
import('kableExtra')
import('tidyverse')
import('RColorBrewer')
import('ggthemes')
import('stargazer')
import('lmtest')
import('maps')
import('summarytools')
```

# Introduction and Research Question

<center>![Counties of North Carolina](assets/north_carolina.png)</center>

The primary motivation of this report is to provide causal estimates for determinants of crime in the state of North Carolina. The main aim of our study is to shape the public policy platforms, of the political campaign that has hired our services, in the upcoming election cycle. We strive to provide actionable policy recommendations through motivated data analysis of the research question(s).

Our research focuses on the following specific question: <b>Can crime rates be reduced by a tougher criminal justice system?</b> We explore how certainty of punishment and severity of punishment within the criminal justice system affects crime. As one of the key goals of the political campaign is to reduce the crime rate, the natural choice for the outcome variable of our study is the Crime Rate variable. We seek to explain the variability of this variable using Probability of Arrests, Probability of Conviction, and Probability of Prison.

$$
\begin{aligned}
\textbf{Crime Rate} & \sim \textbf{Probability of Arrest} \\
           & + \textbf{Probability of Conviction} \\
           & + \textbf{Probability of Prison}
\end{aligned}
$$

We will primarily use these three explanatory variables as proxies to measure the effects of the the criminal justice system on crime; and this relationship will be explored in our first OLS model. However, we expect other variables to have significant secondary effects on this relationship and will further explore their impact in our extended second OLS model.

## Policy Hypothesis

The null hypothesis assumes that the outcome Crime Rate variable is not impacted by the three explanatory variables Probability of Arrest, Probability of Conviction, and Probability of Prison. We will test the following hypothesis through our detailed data analysis to justify our ultimate policy recommendation(s).

+ <b>Increasing arrest rates will decrease crime rate</b>
<br/>Actionable through more technological and manpower resources allocated to law enforcement
+ <b>Increasing conviction rates will decrease crime rate</b>
<br/>Actionable through stricter sentencing guidelines and more resources allocated to law enforcement agencies to collect higher quality evidence
+ <b>Increasing prison verdicts will decrease crime rate</b>
<br/>Actionable through stricter sentencing guidelines for courts

Our recommendations will be based on sound statistically significant and practically significant results. We hope to reject the null hypothesis by sufficiently explaining the variability of Crime Rate by our explanatory variables. (causal vs associative ..?)

# Data Loading and Cleaning

The primary data source for our study is **Cornwell and W. Trumball (1994), â€œEstimating the Economic Model of Crime with Panel Data, Review of Economics and Statistics 76, 360-366**. We will use a single cross-section of data for **1987** from the multi-year panel. (The authors used panel data methods and instrumental variables to control for some types of omitted variables)

The **Cornwell Dataset** is provided for the year **1987** except for the **Percent Minority** (pctmin), which is provided for 1980. (Talk about additional datasets we used and why here?)

The **Cornwell Dataset** contained several technical defects such as empty (NA) rows, a duplicated row, and a typo that prevented a numeric variable from being loaded correctly.

```{r loading}
crime         <- read.csv('crime_v2.csv')
crime         <- na.omit(crime)
crime         <- crime[!duplicated(crime$county),]
crime$prbconv <- as.numeric(as.character(crime$prbconv))

crime.numeric <- crime[, !names(crime) %in% c('county','year','west','central','urban')]

dim(crime)
```


From the 100 counties of North Carolina our dataset contain a sample of **90** counties; it contains **25** metrics from the following aspects.

 - Crime & Law Enforcement
    crmrte, prbarr, prbconv, prbpris, avgsen, polpc, mix
 - Demographics & Geography
    density, west, central, urban
    pctmin80,taxpc, pctymle
 - Wages
    w*(9)

```{r}
smoothScatter(crime$prbarr,crime$crmrte)
crime$crmrte.log <- log(crime$crmrte)

pHist <- function(x)
{
    ggplot() + geom_histogram(aes(y=..density..,x=x), bins=20, fill="yellow", colour="black") +
        geom_vline(xintercept=mean(x),linetype="dashed",size=1,colour="blue") +
        stat_function(aes(x=x), fun = dnorm, colour = "red", size = 1, args = list(mean = mean(x), sd = sd(x))) +
        theme_economist()
}

pHist(crime$crmrte)
pHist(crime$crmrte.log)
```

```{r summary, results='hide'}
#star.1 <- stargazer(crime, align = T, type = 'html', style = 'ajs', notes = 'Univariate Summary')
```

```{r}
dfSummary(crime, style = "grid")
```

<br/>

```{r crime, fig.align = 'center'}
source('assets/maps.R')
map('crmrte', 'Crime Rate per Capita')
```

<br/>

We had to address the following anomalies in the dataset.

+ The *Probability of Conviction* (prbconv) contains *1* observation(s) that are outside the acceptable range for a probability : [0,1]. We may also consider that this variable is mislabled and is indeed not a probability.

+ The *Probability of Arrests* (prbarr) contains *10* observation(s) that are outside the acceptable range for a probability : [0,1].

+ The *1000 People per Square Mile* (density) contains an anamolous value of 0.0000203422 for Swain County (FIPS=173). According to the U.S. Census Bureau the 1987 population for this county was 10,932; and the total area is 541 sq. miles. Therefore the correct density was 20.207. Looks like there was an arithmatic error which we can easily resolved : 0.0203422.

We could assume that the data used to calculate these probabalities were inconsistent and possibly not from the same year and or county which has produced this error. Unfortunately this weakens the verasity of these particular variables.

## Transformations

- Experiment with X transformations. Use higher R.Squared and lower MSE as a guide.
- Y tranformations must be applied universally.

# EDA (Model Building Process)


The outcome variable of our study is (crmrte) which is coded as 'crimes per person'.


- What do you want to measure? Make sure you identify variables that will be relevant to the concerns of the political campaign.

- What covariates help you identify a causal effect?

- What covariates are problematic, either due to multicollinearity, or because they will absorb some of a causal effect you want to measure?

- What transformations should you apply to each variable? This is very important because transformations can reveal linearities in the data, make our results relevant, or help us meet model assumptions.

- Are your choices supported by EDA?


Higher Crime Rate Seems to Indicate Higher Police. I.E. the police presence was enhanced in crime ridden counties.


# Regression Models

## Base Model
## Second Model
## Third Model
## Regression Table

# Ommitted Variables

Possible explanatory variables that are not available :
- Level of education.
- Unemployment.
- Private security.
- Sevirity of crimes; misdemeanours vs felonies.
- Type of crimes; drug offenses vs violent crimes, or white collar crimes vs blue collar crimes etc.

# Conclusion

- Does the conclusion address the high-level concerns of a political campaign?
- Is the discussion connected to whether the key effects are real or whether they may be solely an artifact of omitted variable bias?


```{r mychunk, echo = FALSE, fig.height=3, fig.width=5}

library(knitr)
# sample data
dat <- data.frame(
  text = sapply(1:10, FUN = function(x) { paste0(sample(x = LETTERS, size = 15), collapse = '') }), 
  x1 = rnorm(10), 
  x2 = rnorm(10, mean = 3), 
  x3 = rnorm(10, mean = 5))

# generate plots
invisible(apply(dat[, 2:4], MARGIN = 1, FUN = boxplot))

out <- cbind(row.names(dat), 
             as.character(dat$text), 
             sprintf('![](%s%s-%s.png)', opts_current$get('fig.path'), opts_current$get('label'), 1:nrow(dat)))
kable(out, col.names = c('ID', 'Text', 'Boxplot'))
```

            X   Y~X  LY~X LY~LX
[1] '  prbarr : 0.16 0.22 0.19'
[1] ' prbconv : 0.15 0.20 0.14'
[1] ' prbpris : 0.00 0.00 0.00'
[1] '  avgsen : 0.00 0.00 0.00'
[1] '   polpc : 0.03 0.00 0.08'
[1] ' density : 0.53 0.40 0.24'
[1] '   taxpc : 0.20 0.13 0.12'
[1] 'pctmin80 : 0.03 0.05 0.16'
[1] '    wcon : 0.15 0.16 0.16'
[1] '    wtuc : 0.06 0.04 0.04'
[1] '    wtrd : 0.18 0.16 0.15'
[1] '    wfir : 0.11 0.09 0.08'
[1] '    wser : 0.00 0.01 0.00'
[1] '    wmfg : 0.12 0.09 0.13'
[1] '    wfed : 0.24 0.27 0.25'
[1] '    wsta : 0.04 0.03 0.02'
[1] '    wloc : 0.13 0.08 0.09'
[1] '     mix : 0.02 0.02 0.00'
[1] ' pctymle : 0.08 0.08 0.10'
